# BERT 란?
위키피디아 33억 단어로 Pretraining 되어 있음

### 학습 방법
1. 마스크 러닝 모델: 마스크로 가린 부분을 맞출 수 있는지 
2. 다음 문장 유추: 문맥 학습



---
- 재료비로 GPU 신청 가능
- 최대 100만원 4/3까지



## keras-bert
- sigmoid
긍부정 판별



### 교수님 첨언) 
그래프로 validation, training accuracy 변화 추이 확인
게시판 글 스크레이핑 후 preprocessing



## 주제
자연어 처리
긍정, 부정
확장
구현을 목표로 !
multi-classification (다중 분류)
댓글
사회적 이슈, 현상
기사 연동

ex) 실시간 검색어와 연관있는
구글 트렌드
마스크(한국), 소독약(미국)

글자, 단어, 문장 level



## 생성 모델
GAN







